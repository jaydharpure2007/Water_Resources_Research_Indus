{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d337cf6",
   "metadata": {},
   "source": [
    "## For converting JPL GRACE Mascon data from NetCDF (nc) to TIFF (tif) format\n",
    "__https://grace.jpl.nasa.gov/data/get-data/jpl_global_mascons/__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def JPL_nc_to_tif(netcdf_file, output_dir, csv_path):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Variable name to extract from the NetCDF file\n",
    "    variable_name = 'lwe_thickness'\n",
    "\n",
    "    # Open the NetCDF file for reading\n",
    "    dataset = Dataset(netcdf_file)\n",
    "\n",
    "    # Get the variable data\n",
    "    variable_data = dataset.variables[variable_name]\n",
    "\n",
    "    # Get the fill value or use a default value\n",
    "    nodata_value = variable_data._FillValue if hasattr(variable_data, '_FillValue') else -99999.0\n",
    "\n",
    "    # Get the number of time steps (assuming 'time' is the first dimension)\n",
    "    dim_size = len(dataset.dimensions['time'])\n",
    "\n",
    "    # Read the CSV file for naming TIF files\n",
    "    ls_name = pd.read_csv(csv_path, header=None)\n",
    "\n",
    "    # Get latitude and longitude information from the dataset\n",
    "    lon = dataset.variables['lon'][:]\n",
    "    lat = dataset.variables['lat'][:]\n",
    "    transform = rasterio.transform.from_origin(lon.min(), lat.max(), lon[1] - lon[0], lat[1] - lat[0])\n",
    "\n",
    "    # Loop over each time step\n",
    "    for i in range(dim_size):\n",
    "        # Extract the slice of data for the current time step\n",
    "        variable_slice = variable_data[i][::-1, :]\n",
    "\n",
    "        # Create the TIF file path\n",
    "        output_tif = os.path.join(output_dir, ls_name.iloc[i, 0] + '.tif')\n",
    "\n",
    "        # Write the data to a TIF file\n",
    "        with rasterio.open(output_tif, 'w', driver='GTiff', height=variable_slice.shape[0],\n",
    "                           width=variable_slice.shape[1], count=1, dtype=variable_slice.dtype,\n",
    "                           crs='EPSG:4326', transform=transform, nodata=nodata_value) as dst:\n",
    "            dst.write(variable_slice, 1)\n",
    "\n",
    "        print(f'TIF file saved at: {output_tif}')\n",
    "\n",
    "    dataset.close()\n",
    "\n",
    "netcdf_file = '...your_netcdf_file.nc'\n",
    "output_dir = '...your_output_directory'\n",
    "csv_path = '...your_csv_file.csv' ## provided in the folder\n",
    "\n",
    "JPL_nc_to_tif(netcdf_file, output_dir, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc5590",
   "metadata": {},
   "source": [
    "## Extracting the scaling factor from JPL mascon GRACE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "\n",
    "def JPL_nc_SF_tif(netcdf_file, output_dir, variable_name='scale_factor'):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Open the NetCDF file for reading\n",
    "    dataset = Dataset(netcdf_file)\n",
    "\n",
    "    # Get the variable data (scale factor by default)\n",
    "    variable_data = dataset.variables[variable_name]\n",
    "\n",
    "    # Get the fill value or use a default value\n",
    "    nodata_value = variable_data._FillValue if hasattr(variable_data, '_FillValue') else -99999.0\n",
    "\n",
    "    # Reverse the data for correct orientation (latitude usually needs to be flipped)\n",
    "    variable_slice = variable_data[::-1, :]\n",
    "\n",
    "    # Get the metadata (coordinates, spatial reference system, etc.) from the NetCDF file\n",
    "    lon = dataset.variables['lon'][:]\n",
    "    lat = dataset.variables['lat'][:]\n",
    "    transform = rasterio.transform.from_origin(lon.min(), lat.max(), lon[1] - lon[0], lat[1] - lat[0])\n",
    "\n",
    "    # Create the TIF file path\n",
    "    output_tif = os.path.join(output_dir, f'{variable_name}.tif')\n",
    "\n",
    "    # Write the variable data to a TIF file\n",
    "    with rasterio.open(output_tif, 'w', driver='GTiff', height=variable_slice.shape[0],\n",
    "                            width=variable_slice.shape[1], count=1, dtype=variable_slice.dtype,\n",
    "                            crs='EPSG:4326', transform=transform, nodata=nodata_value) as dst:\n",
    "        dst.write(variable_slice, 1)\n",
    "\n",
    "    print(f'TIF file saved at: {output_tif}')\n",
    "\n",
    "    # Close the NetCDF dataset\n",
    "    dataset.close()\n",
    "\n",
    "netcdf_file = '....your_netcdf_file.nc'\n",
    "output_dir = '...your_output_directory'\n",
    "JPL_nc_SF_tif(netcdf_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddb50a0",
   "metadata": {},
   "source": [
    "## JPL GRACE TWSA = mascon * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ee50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os\n",
    "\n",
    "def JPL_TWSA_SF(input_dir, scale_factor_file, output_dir):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get the list of TIF files in the input directory\n",
    "    list_files = [f for f in os.listdir(input_dir) if f.endswith('.tif')]\n",
    "\n",
    "    # Open and read the scale factor file\n",
    "    with rasterio.open(scale_factor_file) as sf:\n",
    "        sf_data = sf.read(1, masked=True)\n",
    "\n",
    "    # Loop through each file in the list\n",
    "    for fl in list_files:\n",
    "        input_tiff_path = os.path.join(input_dir, fl)\n",
    "\n",
    "        # Open the input TIF file\n",
    "        with rasterio.open(input_tiff_path, 'r') as src:\n",
    "            # Read the raster data\n",
    "            raster_data = src.read(1, masked=True)\n",
    "\n",
    "            # Multiply the raster data with the scale factor\n",
    "            result = raster_data * sf_data\n",
    "\n",
    "            # Copy and update metadata for the output TIF file\n",
    "            meta = src.meta.copy()\n",
    "            meta.update(nodata=src.nodata)\n",
    "\n",
    "        # Define output TIF file path\n",
    "        output_tiff_path = os.path.join(output_dir, fl)\n",
    "\n",
    "        # Write the scaled data to the output TIF file\n",
    "        with rasterio.open(output_tiff_path, 'w', **meta) as dst:\n",
    "            dst.write(result, 1)\n",
    "\n",
    "        # Print the path of the saved file\n",
    "        print(f'Saved file: {output_tiff_path}')\n",
    "\n",
    "input_directory = '...path_to_input_tifs'\n",
    "scale_factor_path = '...path_to_scale_factor_tif'\n",
    "output_directory = '...path_to_output_directory'\n",
    "\n",
    "JPL_TWSA_SF(input_directory, scale_factor_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9d683",
   "metadata": {},
   "source": [
    "## For converting CSR GRACE Mascon data from NetCDF (nc) to TIFF (tif) format\n",
    "__https://grace.jpl.nasa.gov/data/get-data/__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4adbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import os\n",
    "from netCDF4 import num2date\n",
    "\n",
    "def CSR_nc_tif(netcdf_file, output_dir, csv_file):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Variable name to extract from the NetCDF file\n",
    "    variable_name = 'lwe_thickness'\n",
    "\n",
    "    # Open the NetCDF file for reading\n",
    "    dataset = nc.Dataset(netcdf_file)\n",
    "\n",
    "    # Get the variable data\n",
    "    variable_data = dataset.variables[variable_name]\n",
    "    \n",
    "    # Get the nodata value\n",
    "    nodata_value = -99999.0\n",
    "\n",
    "    # Get the dimension size (assuming it represents time)\n",
    "    dim_size = len(dataset.dimensions['time'])\n",
    "\n",
    "    # Reading CSV file containing names for each timestep\n",
    "    ls_name = pd.read_csv(csv_file, header=None)\n",
    "\n",
    "    # Loop over each index of the time dimension\n",
    "    for i in range(dim_size):\n",
    "        # Read the variable data for the current time slice and reverse latitudes\n",
    "        variable_slice = variable_data[i][::-1, :]\n",
    "\n",
    "        # Get the longitude and latitude coordinates from the NetCDF file\n",
    "        lon = dataset.variables['lon'][:]\n",
    "        lat = dataset.variables['lat'][:]\n",
    "\n",
    "        # Define the transform based on the spatial information\n",
    "        transform = rasterio.transform.from_origin(lon.min(), lat.max(), lon[1] - lon[0], lat[1] - lat[0])\n",
    "\n",
    "        # Create the TIF file path using the corresponding name from the CSV file\n",
    "        output_tif = os.path.join(output_dir, f\"{ls_name.iloc[i, 0]}.tif\")\n",
    "\n",
    "        # Create the TIF file and write the variable data\n",
    "        with rasterio.open(output_tif, 'w', driver='GTiff', height=variable_slice.shape[0],\n",
    "                           width=variable_slice.shape[1], count=1, dtype=variable_slice.dtype,\n",
    "                           crs='EPSG:4326', transform=transform, nodata=nodata_value) as dst:\n",
    "            dst.write(variable_slice, 1)\n",
    "\n",
    "        print(f'TIF file saved at: {output_tif}')\n",
    "\n",
    "    # Close the NetCDF dataset\n",
    "    dataset.close()\n",
    "\n",
    "netcdf_file_path = '...path_to_netCDF_file.nc'\n",
    "output_directory = '....path_to_output_directory'\n",
    "csv_file_path = '....path_to_csv_file.csv'\n",
    "\n",
    "CSR_nc_tif(netcdf_file_path, output_directory, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771daee",
   "metadata": {},
   "source": [
    "## For converting GSFC GRACE Mascon data from NetCDF (nc) to TIFF (tif) format\n",
    "__https://earth.gsfc.nasa.gov/geo/data/grace-mascons__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba18df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def GSFC_nc_tif(netcdf_file, output_dir, csv_file):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Variable name to extract from the NetCDF file\n",
    "    variable_name = 'lwe_thickness'\n",
    "\n",
    "    # Open the NetCDF file for reading\n",
    "    dataset = nc.Dataset(netcdf_file)\n",
    "\n",
    "    # Get the variable data\n",
    "    variable_data = dataset.variables[variable_name]\n",
    "\n",
    "    # Set nodata value\n",
    "    nodata_value = -99999.0\n",
    "\n",
    "    # Get the dimension size (assuming it represents time)\n",
    "    dim_size = len(dataset.dimensions['time'])\n",
    "\n",
    "    # Read the CSV file containing names for each time step\n",
    "    ls_name = pd.read_csv(csv_file, header=None)\n",
    "\n",
    "    # Loop over each index of the time dimension\n",
    "    for i in range(dim_size):\n",
    "        # Read the variable data for the current time slice and reverse latitudes\n",
    "        variable_slice = variable_data[i][::-1, :]\n",
    "\n",
    "        # Get longitude and latitude coordinates from the NetCDF file\n",
    "        lon = dataset.variables['lon'][:]\n",
    "        lat = dataset.variables['lat'][:]\n",
    "\n",
    "        # Define the transform based on the spatial information\n",
    "        transform = rasterio.transform.from_origin(lon.min(), lat.max(), lon[1] - lon[0], lat[1] - lat[0])\n",
    "\n",
    "        # Create the TIF file path using the corresponding name from the CSV file\n",
    "        output_tif = os.path.join(output_dir, f\"{ls_name.iloc[i, 0]}.tif\")\n",
    "\n",
    "        # Create the TIF file and write the variable data\n",
    "        with rasterio.open(output_tif, 'w', driver='GTiff', height=variable_slice.shape[0],\n",
    "                           width=variable_slice.shape[1], count=1, dtype=variable_slice.dtype,\n",
    "                           crs='EPSG:4326', transform=transform, nodata=nodata_value) as dst:\n",
    "            dst.write(variable_slice, 1)\n",
    "\n",
    "        print(f'TIF file saved at: {output_tif}')\n",
    "\n",
    "    # Close the NetCDF dataset\n",
    "    dataset.close()\n",
    "\n",
    "netcdf_file = '...path_to_netCDF_file.nc'\n",
    "output_dir = '...path_to_output_directory'\n",
    "csv_file = '...path_to_csv_file.csv'\n",
    "\n",
    "GSFC_nc_tif(netcdf_file, output_dir, csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd6361",
   "metadata": {},
   "source": [
    "## Resampling the tif file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44afc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "import os\n",
    "\n",
    "def resample_tif(input_tif, output_tif, new_resolution):\n",
    "    # Open the original TIFF file\n",
    "    with rasterio.open(input_tif) as src:\n",
    "        # Calculate the new dimensions\n",
    "        width = int(src.width * (src.res[0] / new_resolution))\n",
    "        height = int(src.height * (src.res[1] / new_resolution))\n",
    "\n",
    "        # Define the transform for the new resolution\n",
    "        transform = src.transform * src.transform.scale(\n",
    "            (src.width / width),\n",
    "            (src.height / height)\n",
    "        )\n",
    "\n",
    "        # Read the data from the original TIFF file\n",
    "        data = src.read(\n",
    "            out_shape=(src.count, height, width),\n",
    "            resampling=Resampling.bilinear  # You can change to nearest, cubic, etc.\n",
    "        )\n",
    "\n",
    "        # Update metadata for the output file\n",
    "        meta = src.meta.copy()\n",
    "        meta.update({\n",
    "            'driver': 'GTiff',\n",
    "            'height': height,\n",
    "            'width': width,\n",
    "            'transform': transform,\n",
    "            'crs': src.crs  # Keep the same coordinate reference system\n",
    "        })\n",
    "\n",
    "        # Write the resampled data to a new TIFF file\n",
    "        with rasterio.open(output_tif, 'w', **meta) as dst:\n",
    "            dst.write(data)\n",
    "\n",
    "    print(f'Resampled TIFF saved at: {output_tif}')\n",
    "\n",
    "# Example usage\n",
    "path_folder = r'....\\GRACE data\\JPL'\n",
    "output_folder = os.path.join(path_folder, \"Resample\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "new_resolution = 0.25  # Target resolution in degree\n",
    "\n",
    "# Get a list of TIFF files in the input folder\n",
    "tiff_files = [f for f in os.listdir(input_folder) if f.endswith('.tif')]\n",
    "# Iterate over all TIFF files in the input folder\n",
    "for filename in tiff_files:\n",
    "    input_tif = os.path.join(path_folder, filename)\n",
    "    output_tif = os.path.join(output_folder, filename)\n",
    "    resample_tif(input_tif, output_tif, new_resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b1f57",
   "metadata": {},
   "source": [
    "## Average of three mascon solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722e767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def average_mascon_solutions(JPL_path, CSR_path, GSFC_path, dst_path):\n",
    "    \"\"\"\n",
    "    Function to compute the average of mascon solutions (GeoTIFFs) from JPL, CSR, and GSFC sources.\n",
    "    \n",
    "    Parameters:\n",
    "    - JPL_path: Path to JPL GeoTIFF files.\n",
    "    - CSR_path: Path to CSR GeoTIFF files.\n",
    "    - GSFC_path: Path to GSFC GeoTIFF files.\n",
    "    - dst_path: Output directory where the averaged GeoTIFFs will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of paths for input directories\n",
    "    file_paths = [JPL_path, CSR_path, GSFC_path]\n",
    "    \n",
    "    # Get the list of GeoTIFF files from JPL (assuming all folders have the same file names)\n",
    "    list_files = [f for f in os.listdir(JPL_path) if f.endswith('.tif')]\n",
    "    list_files.sort()  # Sort the files to ensure proper matching\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(dst_path):\n",
    "        os.makedirs(dst_path)\n",
    "    \n",
    "    # Read metadata from the first file to set up the output\n",
    "    with rasterio.open(os.path.join(JPL_path, list_files[0])) as src:\n",
    "        profile = src.profile  # Metadata profile (CRS, dtype, etc.)\n",
    "        dtype = src.dtypes[0]  # Data type\n",
    "        nodata = src.nodatavals[0]  # NoData value\n",
    "        shape = src.shape  # Shape of the raster\n",
    "    \n",
    "    # Loop over each GeoTIFF file\n",
    "    for fl in list_files:\n",
    "        raster_files = [os.path.join(path, fl) for path in file_paths if os.path.exists(os.path.join(path, fl))]\n",
    "        \n",
    "        if len(raster_files) < 3:\n",
    "            print(f\"Skipping {fl} due to missing files in one or more directories.\")\n",
    "            continue\n",
    "        \n",
    "        # Initialize a list to store raster data\n",
    "        image_data = []\n",
    "        \n",
    "        # Read and append data from all three rasters\n",
    "        for file in raster_files:\n",
    "            with rasterio.open(file) as src:\n",
    "                data = src.read(1, masked=True)  # Read band 1 with masking\n",
    "                image_data.append(data)\n",
    "        \n",
    "        # Convert the list of image data to a NumPy masked array and compute the average\n",
    "        image_data = np.ma.array(image_data)\n",
    "        average_image = np.ma.mean(image_data, axis=0)\n",
    "        \n",
    "        # Update metadata profile for the output raster\n",
    "        profile.update(dtype=dtype, nodata=nodata)\n",
    "        \n",
    "        # Define the output file path\n",
    "        output_file = os.path.join(dst_path, fl)\n",
    "        \n",
    "        # Write the averaged data to a new GeoTIFF file\n",
    "        with rasterio.open(output_file, 'w', **profile) as dst:\n",
    "            dst.write(average_image, 1)\n",
    "        \n",
    "        print(f\"Saved averaged GeoTIFF: {output_file}\")\n",
    "\n",
    "# Define paths\n",
    "JPL_path = r\"...\\GRACE data\\JPL\\\\\"\n",
    "CSR_path = r\"...\\GRACE data\\CSR\\\\\"\n",
    "GSFC_path = r\"...\\GRACE data\\GSFC\\\\\"\n",
    "dst_path = r\"...\\GRACE data\\Average\\\\\"\n",
    "\n",
    "# Run the function\n",
    "average_mascon_solutions(JPL_path, CSR_path, GSFC_path, dst_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b25d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbae6ca4",
   "metadata": {},
   "source": [
    "## Create raster layer normalized Day of Year (nDoY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "def normalized_day_of_year(year, month, day):\n",
    "\n",
    "    # Create a datetime object for the input date\n",
    "    date_object = datetime(year, month, day)\n",
    "\n",
    "    # Calculate the day of the year (DOY) as an integer value (ranging from 1 to 365/366)\n",
    "    day_of_year = date_object.timetuple().tm_yday\n",
    "\n",
    "    # Calculate the total number of days in the year\n",
    "    total_days_in_year = 365 if not is_leap_year(year) else 366\n",
    "\n",
    "    # Calculate the normalized day of the year as a fraction (ranging from 0 to 1)\n",
    "    normalized_doy = (day_of_year - 1) / total_days_in_year\n",
    "\n",
    "    return normalized_doy\n",
    "\n",
    "def is_leap_year(year):\n",
    "    # Function to check if a year is a leap year\n",
    "    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n",
    "\n",
    "def array_ndoy(start_date, end_date, row, col):\n",
    "    # Create the date range with a monthly difference starting from the 15th day of each month\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"M\")\n",
    "    # Convert the date range to a list\n",
    "    date_range_list = [str(date).strip('00:00:00').replace(\" \", \"\") for date in date_range.to_list()]\n",
    "\n",
    "    # Creat the array of ndoy\n",
    "    ndoy = np.zeros((len(date_range_list), row, col))\n",
    "    for index, date in enumerate(date_range_list):\n",
    "        for r in range(row):\n",
    "            for c in range(col):\n",
    "                # Convert the string date to a datetime object\n",
    "                date_obj = datetime.strptime(str(date), \"%Y-%m-%d\")\n",
    "                # Extract the components\n",
    "                day = int(date_obj.day/2)\n",
    "                month = date_obj.month\n",
    "                year = date_obj.year\n",
    "\n",
    "                normalized_doy = normalized_day_of_year(year, month, day)\n",
    "                ndoy[index, r, c] = normalized_doy\n",
    "    return ndoy\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "ndoy_path = r'...\\GRACE data\\NDOY\\\\'\n",
    "if not os.path.exists(ndoy_path):\n",
    "    os.makedirs(ndoy_path)\n",
    "\n",
    "ref_path = r'...\\Model data\\ET\\\\'\n",
    "ref_files = [f for f in os.listdir(os.path.join(ref_path)) if f.endswith('.tif')]\n",
    "ref_files.sort()\n",
    "\n",
    "with rasterio.open(os.path.join(ref_path, ref_files[0])) as ref:\n",
    "    input_data = ref.read(1, masked = True)\n",
    "    profile = ref.profile\n",
    "    \n",
    "row, col = input_data.shape\n",
    "# Define the start and end dates\n",
    "start_date = \"2002-04-01\"\n",
    "end_date = \"2023-05-31\"\n",
    "    \n",
    "ndoy = array_ndoy(start_date, end_date, row, col)    \n",
    "n_sample = ndoy.shape[0]    \n",
    "for sample in range(n_sample):\n",
    "    with rasterio.open(ndoy_path+ref_files[sample], \"w\", **profile) as dst:\n",
    "        dst.write(np.array(ndoy[sample]), 1)\n",
    "        print(ref_files[sample])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb164524",
   "metadata": {},
   "source": [
    "## Decompose all input variables into their residual and slope components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import re\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import pymannkendall as mk\n",
    "\n",
    "def create_month_range(start_date, end_date):\n",
    "    \"\"\"Generates a list of monthly dates between start and end dates in 'YYYY-MM' format.\"\"\"\n",
    "    monthly_dates = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        monthly_dates.append(current_date.strftime('%Y-%m'))\n",
    "        current_date += relativedelta(months=1)\n",
    "    return monthly_dates\n",
    "\n",
    "def atoi(text):\n",
    "    \"\"\"Helper function for sorting numbers in text.\"\"\"\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    \"\"\"Natural sorting key.\"\"\"\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "def data_retrieve_withna(fol_files, date_range, array):\n",
    "    \"\"\"Retrieves data, introducing NaN values for missing files in the date range.\"\"\"\n",
    "    data_with_missing = []\n",
    "    i = 0\n",
    "    for date in date_range:\n",
    "        if date + \".tif\" in fol_files:\n",
    "            data_with_missing.append(array[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            data_with_missing.append(np.nan)\n",
    "    return data_with_missing\n",
    "\n",
    "def stl_method(data_without_na, array_miss):\n",
    "    \"\"\"Performs STL decomposition and returns the linear trend and residuals.\"\"\"\n",
    "    stl_result = STL(data_without_na, period=12, seasonal=13, trend_deg=0, seasonal_deg=0, low_pass_deg=0, robust=True).fit()\n",
    "    trend_component = stl_result.trend\n",
    "    mk_result = mk.original_test(trend_component)\n",
    "    \n",
    "    linear_trend = [(x + 1) * mk_result.slope + mk_result.intercept for x in range(len(array_miss))]\n",
    "    residual = np.array(array_miss) - np.array(linear_trend)\n",
    "    \n",
    "    return linear_trend, residual\n",
    "\n",
    "def get_folder_list(path):\n",
    "    \"\"\"Returns a list of subfolders in the specified path.\"\"\"\n",
    "    return [entry.name for entry in os.scandir(path) if entry.is_dir()]\n",
    "\n",
    "def decompose_all_data(path):\n",
    "    folder_list = get_folder_list(path)\n",
    "    \n",
    "    for folder in folder_list:\n",
    "        fol_path = os.path.join(path, folder)\n",
    "        fol_files = sorted([f for f in os.listdir(fol_path) if f.endswith('.tif')], key=natural_keys)\n",
    "\n",
    "        # Read metadata from the first file\n",
    "        with rasterio.open(os.path.join(fol_path, fol_files[0])) as ref:\n",
    "            ref_data = ref.read(1, masked=True)\n",
    "            profile = ref.profile\n",
    "            nodata = ref.nodata\n",
    "\n",
    "        # Date range\n",
    "        start_date = datetime(2002, 4, 1)\n",
    "        end_date = datetime(2023, 5, 31)\n",
    "        date_range = create_month_range(start_date, end_date)\n",
    "\n",
    "        # Read all fol files into an array\n",
    "        raster_array = [rasterio.open(os.path.join(fol_path, filename)).read(1, masked=True) for filename in fol_files]\n",
    "        raster_array = np.array(raster_array)\n",
    "\n",
    "        n_sample, row, col = raster_array.shape\n",
    "\n",
    "        slope_path = os.path.join(fol_path, \"Slope\")\n",
    "        os.makedirs(slope_path, exist_ok=True)\n",
    "\n",
    "        residual_path = os.path.join(gfol_path, \"Residual\")\n",
    "        os.makedirs(residual_path, exist_ok=True)\n",
    "\n",
    "        lt, rd = [], []\n",
    "\n",
    "        for r in range(row):\n",
    "            for c in range(col):\n",
    "                pixel_data = raster_array[:, r, c]\n",
    "                if pixel_data[0] != nodata:\n",
    "                    array_miss = data_retrieve_withna(fol_files, date_range, pixel_data)\n",
    "                    data_without_na = [x for x in array_miss if not np.isnan(x)]\n",
    "                    linear_trend, residual = stl_method(data_without_na, array_miss)\n",
    "                    lt.append(linear_trend)\n",
    "                    rd.append(residual)\n",
    "                else:\n",
    "                    lt.append([nodata] * len(date_range))\n",
    "                    rd.append([nodata] * len(date_range))\n",
    "\n",
    "        lt = np.array(lt).reshape(row, col, len(date_range))\n",
    "        rd = np.array(rd).reshape(row, col, len(date_range))\n",
    "\n",
    "        # Write output files\n",
    "        for i, date in enumerate(date_range):\n",
    "            slope_file = os.path.join(slope_path, f\"{date}.tif\")\n",
    "            residual_file = os.path.join(residual_path, f\"{date}.tif\")\n",
    "            \n",
    "            with rasterio.open(slope_file, 'w', **profile) as dst:\n",
    "                dst.write(lt[:, :, i], 1)\n",
    "            with rasterio.open(residual_file, 'w', **profile) as dst:\n",
    "                dst.write(rd[:, :, i], 1)\n",
    "            \n",
    "            print(f\"Processed {date} successfully.\")\n",
    "\n",
    "# Define path \n",
    "path = r\"...\\Model without slope\\Data\\\\\"\n",
    "decompose_all_data(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d374f1",
   "metadata": {},
   "source": [
    "# For the decomposition of GRACE data into slope and residual components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc93ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import re\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import pymannkendall as mk\n",
    "\n",
    "# Function to create a monthly date range between start and end dates\n",
    "def create_month_range(start_date, end_date):\n",
    "    current_date = start_date\n",
    "    monthly_dates = []\n",
    "    while current_date <= end_date:\n",
    "        monthly_dates.append(current_date.strftime('%Y-%m'))\n",
    "        current_date += relativedelta(months=1)\n",
    "    return monthly_dates\n",
    "\n",
    "# Sorting function for filenames\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "# Function to retrieve data, filling missing values with NaN\n",
    "def data_retrieve_withna(GRACE_files, date_range, array):\n",
    "    data_with_missing = []\n",
    "    i = 0\n",
    "    for file in date_range:\n",
    "        if file in GRACE_files:\n",
    "            data_with_missing.append(array[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            data_with_missing.append(np.nan)\n",
    "    return data_with_missing\n",
    "\n",
    "# Function to apply the STL decomposition and calculate trends\n",
    "def stl_method(data_without_na, array_miss):\n",
    "    decomposition = STL(data_without_na, period=12, seasonal=13, trend_deg=0, seasonal_deg=0, low_pass_deg=0, robust=True).fit()\n",
    "    trend_component = decomposition.trend\n",
    "\n",
    "    # Perform Mann-Kendall trend test\n",
    "    mk_result = mk.original_test(trend_component)\n",
    "    linear_trend = [(x + 1) * mk_result.slope + mk_result.intercept for x in range(len(array_miss))]\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals = np.array(array_miss) - np.array(linear_trend)\n",
    "    \n",
    "    return linear_trend, residuals\n",
    "\n",
    "def decompose_grace_data(GRACE_path):\n",
    "    # read GRACE files\n",
    "    GRACE_files = [f for f in os.listdir(GRACE_path) if f.endswith('.tif')]\n",
    "    GRACE_files.sort(key=natural_keys)\n",
    "\n",
    "    # Read reference data and set metadata profile\n",
    "    with rasterio.open(os.path.join(GRACE_path, GRACE_files[0])) as ref:\n",
    "        ref_data = ref.read(1, masked=True)\n",
    "        profile = ref.profile\n",
    "        nodata = ref.nodata\n",
    "\n",
    "    # Define start and end dates\n",
    "    start_date = datetime(2002, 4, 1)\n",
    "    end_date = datetime(2023, 5, 31)\n",
    "\n",
    "    # Create date range for the files\n",
    "    date_range = create_month_range(start_date, end_date)\n",
    "    date_range_files = [f + \".tif\" for f in date_range]\n",
    "\n",
    "    # Read all GRACE raster data into a list\n",
    "    raster_array = []\n",
    "    for filename in GRACE_files:\n",
    "        with rasterio.open(os.path.join(GRACE_path, filename)) as src:\n",
    "            raster_array.append(src.read(1, masked=True))\n",
    "\n",
    "    # Reshape the raster array\n",
    "    n_sample, row, col = np.array(raster_array).shape\n",
    "\n",
    "    # Create directories for output if they don't exist\n",
    "    slope_path = os.path.join(GRACE_path, \"Slope\")\n",
    "    residual_path = os.path.join(GRACE_path, \"Residual\")\n",
    "    os.makedirs(slope_path, exist_ok=True)\n",
    "    os.makedirs(residual_path, exist_ok=True)\n",
    "\n",
    "    # Initialize lists for linear trends and residuals\n",
    "    lt = []\n",
    "    rd = []\n",
    "\n",
    "    # Process each pixel\n",
    "    for r in range(row):\n",
    "        for c in range(col):\n",
    "            if np.array(raster_array)[0, r, c] != nodata:\n",
    "                array_miss = data_retrieve_withna(GRACE_files, date_range_files, np.array(raster_array)[:, r, c])\n",
    "                data_without_na = [x for x in array_miss if not np.isnan(x)]\n",
    "                linear_trend, residual = stl_method(data_without_na, array_miss)\n",
    "            else:\n",
    "                array_miss = [nodata for _ in range(len(date_range_files))]\n",
    "                linear_trend, residual = np.array(array_miss), np.array(array_miss)\n",
    "\n",
    "            lt.append(linear_trend)\n",
    "            rd.append(residual)\n",
    "\n",
    "    # Reshape lists to arrays\n",
    "    lt = np.array(lt).reshape(row, col, len(date_range_files))\n",
    "    rd = np.array(rd).reshape(row, col, len(date_range_files))\n",
    "\n",
    "    # Save linear trend and residual rasters\n",
    "    for i in range(len(date_range_files)):\n",
    "        with rasterio.open(os.path.join(slope_path, date_range_files[i]), \"w\", **profile) as dst:\n",
    "            dst.write(lt[:, :, i], 1)\n",
    "        if date_range_files[i] in GRACE_files:\n",
    "            with rasterio.open(os.path.join(residual_path, date_range_files[i]), \"w\", **profile) as dst:\n",
    "                dst.write(rd[:, :, i], 1)\n",
    "        print(f\"{i} - {date_range_files[i]} saved successfully\")\n",
    "\n",
    "        # Define paths \n",
    "path = r\"...\\GRACE TWSA\\\\\"\n",
    "decompose_grace_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02811d",
   "metadata": {},
   "source": [
    "## Apply the MissForest model for filling spatial data gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd83b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from osgeo import gdal\n",
    "import re\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from missingpy import KNNImputer, MissForest\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# Helper function to convert strings to integers if applicable\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "# Function to create natural sorting keys\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "# Function to retrieve a list of folders from a given path\n",
    "def get_folder_list(path):\n",
    "    folder_list = [entry.name for entry in os.scandir(path) if entry.is_dir()]\n",
    "    return folder_list\n",
    "\n",
    "# Main path for data\n",
    "path = r'...\\Model without slope\\Data\\\\'\n",
    "\n",
    "# Get the list of variable folders and remove the target variable folder ('GRACE TWSA')\n",
    "vars_list = get_folder_list(path)\n",
    "target_var = 'GRACE TWSA'\n",
    "vars_list.remove(target_var)\n",
    "input_vars = vars_list\n",
    "print(input_vars)\n",
    "\n",
    "# Get and sort target files\n",
    "target_files = [f for f in os.listdir(os.path.join(path, target_var)) if f.endswith('.tif')]\n",
    "target_files.sort(key=natural_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175643c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_target_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9671c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7356d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from missingpy import MissForest\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# Helper function to convert strings to integers if applicable\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "# Function to create natural sorting keys for filenames\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "# Function to retrieve a list of folders from a given path\n",
    "def get_folder_list(path):\n",
    "    return [entry.name for entry in os.scandir(path) if entry.is_dir()]\n",
    "\n",
    "# Main path for data\n",
    "path = r'...\\Model without slope\\Data\\\\'\n",
    "\n",
    "# Get the list of variable folders and remove the target variable folder ('GRACE TWSA')\n",
    "vars_list = get_folder_list(path)\n",
    "target_var = 'GRACE TWSA'\n",
    "vars_list.remove(target_var)  # Remove the target variable from input vars\n",
    "input_vars = vars_list\n",
    "print(f\"Input variables: {input_vars}\")\n",
    "\n",
    "# Get and sort target files\n",
    "target_files = [f for f in os.listdir(os.path.join(path, target_var)) if f.endswith('.tif')]\n",
    "target_files.sort(key=natural_keys)\n",
    "\n",
    "# Get and sort input variable 1 files (assuming the first folder in input_vars is needed)\n",
    "var1_files = [f for f in os.listdir(os.path.join(path, input_vars[0])) if f.endswith('.tif')]\n",
    "var1_files.sort(key=natural_keys)\n",
    "\n",
    "# Initialize the dictionary to store data arrays\n",
    "data_array = {}\n",
    "\n",
    "# Reference data for initializing raster shape and profile\n",
    "with rasterio.open(os.path.join(path, target_var, target_files[0])) as ref:\n",
    "    ref_data = ref.read(1, masked=True)  # Read masked raster data (NaN where no data)\n",
    "    profile = ref.profile  # Save the profile for later use (if needed)\n",
    "\n",
    "# Loop through the input variable 1 files and process each file\n",
    "for index, filename in enumerate(var1_files):\n",
    "    raster_array = []\n",
    "\n",
    "    # Check if the file exists in the target files\n",
    "    if filename in target_files:\n",
    "        for var in input_vars:\n",
    "            # Read input images from respective folders using rasterio\n",
    "            with rasterio.open(os.path.join(path, var, filename)) as src:\n",
    "                input_data = src.read(1, masked=True)  # Read data as masked array\n",
    "                raster_array.append(input_data)\n",
    "        \n",
    "        # Read the target data\n",
    "        with rasterio.open(os.path.join(path, target_var, filename)) as src_t:\n",
    "            target_data = src_t.read(1, masked=True)\n",
    "            raster_array.append(target_data)\n",
    "        \n",
    "        # Stack the input and target data along the third dimension\n",
    "        data_array[index] = np.stack(raster_array, axis=0)\n",
    "    else:\n",
    "        for var in input_vars:\n",
    "            # Read input data for missing target file\n",
    "            with rasterio.open(os.path.join(path, var, filename)) as src:\n",
    "                input_data = src.read(1, masked=True)\n",
    "                raster_array.append(input_data)\n",
    "        \n",
    "        # Create an array of NaNs for missing target data\n",
    "        target_data = np.full(ref_data.shape, np.nan)\n",
    "        raster_array.append(target_data)\n",
    "        \n",
    "        # Stack the input data along with NaN target data\n",
    "        data_array[index] = np.stack(raster_array, axis=0)\n",
    "\n",
    "# Optional: Print the keys of data_array to confirm files processed\n",
    "print(f\"Processed data indices: {list(data_array.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbeef5",
   "metadata": {},
   "source": [
    "## Data standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca71234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the keys from the data array and get shape parameters\n",
    "data_samples = list(data_array.keys())\n",
    "n_features, row, col = data_array[0].shape\n",
    "\n",
    "print('n_feature:', n_features, 'Row:', row, 'Col:', col)\n",
    "\n",
    "# Initialize dictionaries and lists for normalized data, mean, and standard deviation\n",
    "data_norm = {}\n",
    "data_mean_array = []\n",
    "data_std_array = []\n",
    "\n",
    "# Normalize data for each feature\n",
    "for feature in range(n_features):\n",
    "    for r in range(row):\n",
    "        for c in range(col):\n",
    "            data_temp = [data_array[sample][feature, r, c] for sample in data_samples]\n",
    "            \n",
    "            if feature != n_features - 1:\n",
    "                # Normalize data only if not the last feature\n",
    "                if np.array(data_temp)[0] != 9999.0:\n",
    "                    data_mean = np.mean(data_temp)\n",
    "                    data_std = np.std(data_temp)\n",
    "                    if data_std != 0.0:\n",
    "                        data_normalized = (data_temp - data_mean) / data_std\n",
    "                    else:\n",
    "                        data_normalized = data_temp - data_mean\n",
    "                    data_norm[(feature, r, c)] = data_normalized\n",
    "                else:\n",
    "                    data_norm[(feature, r, c)] = np.array(data_temp)\n",
    "            else:\n",
    "                # Special case for the last feature\n",
    "                if np.array(data_temp)[0] != -99999.0:\n",
    "                    data_mean = np.nanmean(data_temp)\n",
    "                    data_std = np.nanstd(data_temp)\n",
    "                    data_mean_array.append(data_mean)\n",
    "                    if data_std != 0.0:\n",
    "                        data_normalized = (data_temp - data_mean) / data_std\n",
    "                    else:\n",
    "                        data_normalized = data_temp - data_mean\n",
    "                    data_std_array.append(data_std)\n",
    "                    data_norm[(feature, r, c)] = data_normalized\n",
    "                else:\n",
    "                    # Handle the case where the first element is -99999.0\n",
    "                    data_mean_array.append(np.array(data_temp)[0])\n",
    "                    data_std_array.append(np.array(data_temp)[0])\n",
    "                    data_norm[(feature, r, c)] = np.array(data_temp)\n",
    "\n",
    "# Prepare normalized images and mean/std arrays for further processing\n",
    "data_norm_image = np.array([data_norm[k] for k in data_norm.keys()]).reshape(n_features, row, col, -1)\n",
    "data_mean_array = np.array(data_mean_array).reshape(row, col)\n",
    "data_std_array = np.array(data_std_array).reshape(row, col)\n",
    "\n",
    "# Optional: Print shapes of normalized images, means, and stds for confirmation\n",
    "print('Normalized image shape:', data_norm_image.shape)\n",
    "print('Mean array shape:', data_mean_array.shape)\n",
    "print('Std array shape:', data_std_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2879994",
   "metadata": {},
   "source": [
    "## MissForest with standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33406ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from missingpy import MissForest\n",
    "\n",
    "# Define the output directory for imputed data\n",
    "imputed_path = r'...\\Model without slope\\MissForest\\\\' \n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(imputed_path, exist_ok=True)\n",
    "\n",
    "# Initialize the MissForest imputer\n",
    "imputer = MissForest()\n",
    "n_features, row, col, n_sample = data_norm_image.shape\n",
    "imputed = []\n",
    "\n",
    "# Iterate through each pixel (row, col) to perform imputation\n",
    "for r in range(row):\n",
    "    for c in range(col):\n",
    "        data = data_norm_image[:, r, c, :].T  # Transpose for easier processing\n",
    "\n",
    "        # Check if the first element is valid\n",
    "        if data[0, 0] != 9999.0:\n",
    "            # Perform imputation\n",
    "            temp = imputer.fit_transform(data)  \n",
    "            # Scale the imputed last layer back to original units\n",
    "            imputed_value = (temp[:, -1] * data_std_array[r, c] + data_mean_array[r, c])\n",
    "        else:\n",
    "            # Handle missing data by replacing NaNs with the mean value\n",
    "            temp_na = data[:, -1]\n",
    "            temp_na[np.isnan(temp_na)] = np.nanmean(temp_na)  # Replace NaNs with the mean value\n",
    "            imputed_value = temp_na\n",
    "\n",
    "        imputed.append(imputed_value)\n",
    "\n",
    "# Convert the list of imputed values to a NumPy array\n",
    "imputed_array = np.array(imputed)  # Shape: [row*col, n_sample]\n",
    "\n",
    "# Update the profile for output raster files\n",
    "profile.update(dtype='float64')\n",
    "\n",
    "# Write each imputed sample to a raster file\n",
    "for sample in range(n_sample):\n",
    "    temp = imputed_array[:, sample].reshape(row, col)\n",
    "    output_file_path = os.path.join(imputed_path, var1_files[sample])\n",
    "    with rasterio.open(output_file_path, \"w\", **profile) as dst:\n",
    "        dst.write(temp, 1)\n",
    "        print(f'Written file: {var1_files[sample]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8b014",
   "metadata": {},
   "source": [
    "## Added slope to the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc1fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define paths for slope and filled data\n",
    "path_slope = r\"...\\Model without slope\\GRACE TWSA\\Slope\\\\\"\n",
    "path_filled_missforest = r\"...\\MissForest\\\\\"\n",
    "\n",
    "# List of input GeoTIFF files\n",
    "list_files = sorted([f for f in os.listdir(path_slope) if f.endswith('.tif')], key=natural_keys)\n",
    "\n",
    "# Destination path for output files\n",
    "path_filled_with_slope = os.path.join(path_filled_missforest, \"Filled\")\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(path_filled_with_slope, exist_ok=True)\n",
    "\n",
    "# Process each file\n",
    "for f in list_files:\n",
    "    # Construct file paths\n",
    "    slope_file = os.path.join(path_slope, f)\n",
    "    filled_file = os.path.join(path_filled_missforest, f)\n",
    "\n",
    "    # Read slope data\n",
    "    with rasterio.open(slope_file) as sl:\n",
    "        slp = sl.read(1)\n",
    "\n",
    "    # Read filled data\n",
    "    with rasterio.open(filled_file) as fl:\n",
    "        flp = fl.read(1)\n",
    "\n",
    "    # Create a mask for NoData values\n",
    "    nodata_mask = (flp == fl.nodata)\n",
    "\n",
    "    # Add the slope to the filled data, excluding NoData values\n",
    "    result = np.where(nodata_mask, flp, slp + flp)\n",
    "\n",
    "    # Update metadata and specify NoData value\n",
    "    meta = fl.meta.copy()\n",
    "    meta.update(nodata=fl.nodata)\n",
    "\n",
    "    # Create a new GeoTIFF file to store the result\n",
    "    output_file = os.path.join(path_filled_with_slope, f)\n",
    "    with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "        dst.write(result.astype(meta['dtype']), 1)\n",
    "\n",
    "    print(f'Written output file: {output_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db3586",
   "metadata": {},
   "source": [
    "## Plot filled with imputed values and actual GRACE TWSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16067b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "def plot_figures(results_data):\n",
    "    plt_row = len(results_data.keys())\n",
    "    # Set up the figure size\n",
    "    figure_size = (12, 4)  # Width, Height\n",
    "    fig, ax = plt.subplots(figsize=figure_size)\n",
    "    fig.tight_layout(pad=4)  # Set the padding in inches\n",
    "\n",
    "    # Extract the data for plotting\n",
    "    y = np.array(results_data['MissForest'][0])\n",
    "    x = np.arange(1, len(y) + 1)  # Generate x values\n",
    "    valid_indices = np.where(np.isnan(y))[0] + 1  # Identify NaN values for valid indices\n",
    "\n",
    "    # Plot predicted and observed values\n",
    "    ax.plot(x, np.array(results_data['MissForest'][1]), color='r', linestyle='-', marker='*', linewidth=1.5, label='Predicted')\n",
    "    ax.plot(x, y, color='b', marker='o', linewidth=1.5, linestyle='-', label='Observed')\n",
    "    \n",
    "    # Add bars to indicate gaps in data\n",
    "    ax.bar(valid_indices, y[valid_indices - 1] - 0.1 * max(y), width=0.8, color='maroon', alpha=0.3, label='Gap in Data')\n",
    "    ax.bar(valid_indices, y[valid_indices - 1] + 0.1 * max(y), width=0.8, color='maroon', alpha=0.3)\n",
    "\n",
    "    # Configure axes\n",
    "    ax.set_xlim(-1, len(y) + 1)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3)\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(10))\n",
    "    ax.set_ylabel('EWH [cm]', fontsize=12)\n",
    "    ax.set_xlabel('Time (months)', fontsize=12)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Save the graph as TIFF with 300 DPI\n",
    "    save_path = r'...\\Indus model\\\\'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    fig.savefig(os.path.join(save_path, 'MissForest_complete.tif'), dpi=300, format='tif', bbox_inches='tight')\n",
    "    \n",
    "    plt.show()               \n",
    "\n",
    "# Set paths for actual and filled data\n",
    "path_actual = r\"...\\Model without slope\\GRACE TWSA\\\\\"\n",
    "path_filled = path_filled_with_slope  # Assuming `path_filled_with_slope` is defined earlier\n",
    "\n",
    "# List actual files\n",
    "actual_files = sorted([f for f in os.listdir(path_actual) if f.endswith('.tif')], key=natural_keys)\n",
    "\n",
    "# List filled files\n",
    "list_files = sorted([f for f in os.listdir(path_slope) if f.endswith('.tif')], key=natural_keys)\n",
    "\n",
    "mean_actual = []\n",
    "mean_filled = []\n",
    "results_com = {}\n",
    "\n",
    "# Calculate means for actual and filled data\n",
    "for file in list_files:\n",
    "    if file in actual_files:\n",
    "        mean_actual.append(calculate_mean_value(os.path.join(path_actual, file)))\n",
    "    else:\n",
    "        mean_actual.append(np.nan)\n",
    "        \n",
    "    mean_filled.append(calculate_mean_value(os.path.join(path_filled, file)))\n",
    "\n",
    "# Stack the results\n",
    "results_com[fol] = np.stack((mean_actual, mean_filled), axis=0)\n",
    "\n",
    "# Plot the figures\n",
    "plot_figures(results_com)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "missingpy​",
   "language": "python",
   "name": "jay"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
